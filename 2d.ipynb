{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7283d10d-aa63-41e6-83f1-4a4f9ca49301",
   "metadata": {
    "id": "7283d10d-aa63-41e6-83f1-4a4f9ca49301"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import shutil\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers import (\n",
    "    AveragePooling2D,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPooling2D,\n",
    ")\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.math import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2016d69-e68c-4dc6-b3e6-6abbfc87f335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "nblog = open(\"data/nb.log\", \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RZY9ZPQDQQRp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZY9ZPQDQQRp",
    "outputId": "41ffaf66-e330-43d4-b350-885cf731b8b2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bAwmi47X2vX",
   "metadata": {
    "id": "2bAwmi47X2vX"
   },
   "outputs": [],
   "source": [
    "!unzip -qq drive/MyDrive/data/images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962b0dee-391d-4c85-8880-3ff01e5b2938",
   "metadata": {
    "id": "962b0dee-391d-4c85-8880-3ff01e5b2938"
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "DATA_DIR = Path(\"data\")\n",
    "#DATA_DIR = Path(\"/content/drive/MyDrive/data\")\n",
    "#IMG_DIR = Path(\"/content/images\")\n",
    "IMG_DIR = DATA_DIR / \"images\"\n",
    "TRAIN_DIR = IMG_DIR / \"train\"\n",
    "TEST_DIR = IMG_DIR / \"test\"\n",
    "US8K_DIR = DATA_DIR / \"UrbanSound8K\"\n",
    "AUDIO_DIR = US8K_DIR / \"audio\"\n",
    "META_CSV = US8K_DIR / \"metadata\" / \"UrbanSound8K.csv\"\n",
    "LOG_DIR = DATA_DIR / \"logs\"\n",
    "NUM_CLASSES = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a70e99-aa4a-436c-b34d-a01c71400f11",
   "metadata": {
    "id": "94a70e99-aa4a-436c-b34d-a01c71400f11"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "SAMPLING_RATE = 16000  # paper: 16000, other values: 22050, 44100\n",
    "CHUNK_SIZE = int(1 * SAMPLING_RATE)  # paper: 16000 (i.e. 1 second), others: 0.1 second\n",
    "OVERLAP_PERCENT = 75  # paper: 75%\n",
    "\n",
    "IMG_HEIGHT = IMG_WIDTH = 72\n",
    "\n",
    "# model related\n",
    "BATCH_SIZE = 100  # paper: 100\n",
    "EPOCHS = 100  # paper: 100\n",
    "EARLY_STOP_PATIENCE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99ec4311",
   "metadata": {
    "id": "99ec4311"
   },
   "outputs": [],
   "source": [
    "# calculated constants\n",
    "STRIDE = int((1 - OVERLAP_PERCENT / 100) * CHUNK_SIZE)\n",
    "MODEL_ID = f\"Sr{SAMPLING_RATE}Cs{CHUNK_SIZE}Ol{OVERLAP_PERCENT}\"\n",
    "FOLD_DATA_DIR = DATA_DIR / f\"foldData.{MODEL_ID}.pickle\"\n",
    "FOLD_DATA = DATA_DIR / f\"foldData.{MODEL_ID}.pickle\"\n",
    "MODEL_FILE = DATA_DIR / \"2d_saved_models\" / f\"model.{MODEL_ID}\"\n",
    "VGG_MODEL_FILE = DATA_DIR / \"vgg_saved_models\" / f\"model.{MODEL_ID}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c319ef-b123-403c-b391-80a904e84771",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9c319ef-b123-403c-b391-80a904e84771",
    "outputId": "ab79013e-4a3d-4032-e1cb-21e4da1ea358"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,\n",
       " 16000,\n",
       " 4000,\n",
       " PosixPath('data/foldData.Sr16000Cs16000Ol75.pickle'),\n",
       " PosixPath('data/2d_saved_models/model.Sr16000Cs16000Ol75'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLING_RATE, CHUNK_SIZE, STRIDE, FOLD_DATA, MODEL_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53271a5-00df-418f-83b0-0af54eceb852",
   "metadata": {
    "id": "e53271a5-00df-418f-83b0-0af54eceb852"
   },
   "outputs": [],
   "source": [
    "def read_chunks():\n",
    "    fold_Xs, fold_ys, fold_chunk_lens = [], [], []\n",
    "    for fold in range(1, 11):\n",
    "        with (FOLD_DATA_DIR / f\"{fold}\").open(\"br\") as f:\n",
    "            X, y, chunk_lens = pickle.load(f)\n",
    "            fold_Xs.append(X)\n",
    "            fold_ys.append(y)\n",
    "            fold_chunk_lens.append(chunk_lens)\n",
    "    return fold_Xs, fold_ys, fold_chunk_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897c6e71",
   "metadata": {
    "id": "897c6e71"
   },
   "outputs": [],
   "source": [
    "# folder structure:\n",
    "# data/images/fold{i}/{classNumber}/{idx}.png\n",
    "def create_melspec_image(fold, fold_X, fold_y, height=IMG_HEIGHT, width=IMG_WIDTH):\n",
    "    fold_dir = IMG_DIR / f\"fold{fold}\"\n",
    "\n",
    "    dpi = plt.rcParams[\"figure.dpi\"]\n",
    "    figsize = (height / dpi, width / dpi)\n",
    "\n",
    "    for idx in range(len(fold_X)):\n",
    "        img_path = fold_dir / f\"{fold_y[idx].argmax()}\" / f\"fold{fold}-{idx}.jpg\"\n",
    "        if not img_path.exists():\n",
    "            mel = librosa.feature.melspectrogram(y=fold_X[idx].reshape(CHUNK_SIZE))\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "            ax.axis(\"off\")\n",
    "            librosa.display.specshow(librosa.power_to_db(mel, ref=np.max), ax=ax)\n",
    "\n",
    "            # https://stackoverflow.com/a/65469535\n",
    "            fig.subplots_adjust(\n",
    "                top=1.0, bottom=0, right=1.0, left=0, hspace=0, wspace=0\n",
    "            )\n",
    "            img_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            fig.savefig(img_path)\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b173fcc4",
   "metadata": {
    "id": "b173fcc4"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    # build network topology\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st conv layer\n",
    "    model.add(Conv2D(16, (3, 3), activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 2nd conv layer\n",
    "    model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 3rd conv layer\n",
    "    model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # flatten output and feed it into dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # output layer\n",
    "    # model.add(keras.layers.Dense(len(NAMES), activation='softmax'))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5cc10fc",
   "metadata": {
    "id": "f5cc10fc"
   },
   "outputs": [],
   "source": [
    "# copy data from folds except test_fold to train directory and others to test directory\n",
    "def copy_to_train_test_dir(test_fold):\n",
    "    shutil.rmtree(TRAIN_DIR, ignore_errors=True)\n",
    "    shutil.rmtree(TEST_DIR, ignore_errors=True)\n",
    "    for fold in range(1, 11):\n",
    "        fold_dir = IMG_DIR / f\"fold{fold}\"\n",
    "        for classno in range(NUM_CLASSES):\n",
    "            src_dir = fold_dir / f\"{classno}\"\n",
    "            dst_dir = (TEST_DIR if fold == test_fold else TRAIN_DIR) / f\"{classno}\"\n",
    "            dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "            for file in src_dir.iterdir():\n",
    "                shutil.copy(file, dst_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea446ec",
   "metadata": {
    "id": "0ea446ec"
   },
   "outputs": [],
   "source": [
    "def sum_rule_agg(y, chunk_lens):\n",
    "    return np.array(\n",
    "        [res.mean(axis=0).argmax() for res in np.split(y, chunk_lens.cumsum()[:-1])]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2c2cd19-b1e6-4230-9a66-9f874c715e27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "b2c2cd19-b1e6-4230-9a66-9f874c715e27",
    "outputId": "277f27cd-10a7-4f5e-aec7-27660f991325"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>99812-1-2-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>159.522205</td>\n",
       "      <td>163.522205</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>99812-1-3-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>181.142431</td>\n",
       "      <td>183.284976</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>99812-1-4-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>242.691902</td>\n",
       "      <td>246.197885</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730</th>\n",
       "      <td>99812-1-5-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>253.209850</td>\n",
       "      <td>255.741948</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>99812-1-6-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>332.289233</td>\n",
       "      <td>334.821332</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8732 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         slice_file_name    fsID       start         end  salience  fold  \\\n",
       "0       100032-3-0-0.wav  100032    0.000000    0.317551         1     5   \n",
       "1     100263-2-0-117.wav  100263   58.500000   62.500000         1     5   \n",
       "2     100263-2-0-121.wav  100263   60.500000   64.500000         1     5   \n",
       "3     100263-2-0-126.wav  100263   63.000000   67.000000         1     5   \n",
       "4     100263-2-0-137.wav  100263   68.500000   72.500000         1     5   \n",
       "...                  ...     ...         ...         ...       ...   ...   \n",
       "8727     99812-1-2-0.wav   99812  159.522205  163.522205         2     7   \n",
       "8728     99812-1-3-0.wav   99812  181.142431  183.284976         2     7   \n",
       "8729     99812-1-4-0.wav   99812  242.691902  246.197885         2     7   \n",
       "8730     99812-1-5-0.wav   99812  253.209850  255.741948         2     7   \n",
       "8731     99812-1-6-0.wav   99812  332.289233  334.821332         2     7   \n",
       "\n",
       "      classID             class  \n",
       "0           3          dog_bark  \n",
       "1           2  children_playing  \n",
       "2           2  children_playing  \n",
       "3           2  children_playing  \n",
       "4           2  children_playing  \n",
       "...       ...               ...  \n",
       "8727        1          car_horn  \n",
       "8728        1          car_horn  \n",
       "8729        1          car_horn  \n",
       "8730        1          car_horn  \n",
       "8731        1          car_horn  \n",
       "\n",
       "[8732 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv(META_CSV)\n",
    "meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cca794f7",
   "metadata": {
    "id": "cca794f7",
    "outputId": "7421ee63-3f10-4700-adf7-7838df44216d"
   },
   "outputs": [],
   "source": [
    "fold_Xs, fold_ys, fold_chunk_lens = read_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886b33a",
   "metadata": {
    "id": "9886b33a"
   },
   "outputs": [],
   "source": [
    "# set _dir_candidates = [] in\n",
    "# /data/arpank/miniforge3/envs/dlproject/lib/python3.10/multiprocessing/heap.py\n",
    "with ProcessPoolExecutor() as e:\n",
    "    for fold in range(1, 11):\n",
    "        e.submit(\n",
    "            create_melspec_image,\n",
    "            fold,\n",
    "            fold_Xs[fold - 1],\n",
    "            fold_ys[fold - 1],\n",
    "        )\n",
    "# takes 30mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43322acd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43322acd",
    "outputId": "20036701-da95-4d49-9fe2-236a67361d28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure GPUs are available\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "gpus\n",
    "# tf.config.set_visible_devices(gpus[2:], \"GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p-ZfAOCppGcj",
   "metadata": {
    "id": "p-ZfAOCppGcj"
   },
   "outputs": [],
   "source": [
    "copy_to_train_test_dir(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d236e",
   "metadata": {
    "id": "968d236e"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24906a08-8351-4a6e-801a-e865b88f8c7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24906a08-8351-4a6e-801a-e865b88f8c7c",
    "outputId": "707df7e4-3e8d-4522-be7f-b9a5912b26c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90030 images belonging to 10 classes.\n",
      "Found 9612 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    #TRAIN_DIR, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE\n",
    "    #TRAIN_DIR, target_size=(224, 224), batch_size=BATCH_SIZE\n",
    "    TRAIN_DIR, target_size=(72, 72), batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    #target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    #target_size=(224, 224),\n",
    "    target_size=(72, 72),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dee4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "264dee4b",
    "outputId": "af464cfe-2b9d-40ed-b43c-dd6b9c2f88b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 70, 70, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 35, 35, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 35, 35, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 33, 33, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 17, 17, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 17, 17, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                262208    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 286,890\n",
      "Trainable params: 286,666\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model = build_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "model = build_model(input_shape=(72, 72, 3))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5a7cd",
   "metadata": {
    "id": "9cc5a7cd"
   },
   "outputs": [],
   "source": [
    "# callbacks to save, stop early and visualize\n",
    "cpcallback = ModelCheckpoint(\n",
    "    monitor=\"val_accuracy\", filepath=MODEL_FILE, save_best_only=True, verbose=1\n",
    ")\n",
    "escallback = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", min_delta=0, patience=10, verbose=1\n",
    ")\n",
    "tbcallback = TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "callbacks = [cpcallback, escallback, tbcallback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaef62a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "beaef62a",
    "outputId": "d19b32b3-a3e1-4660-9065-da0126e40aa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.9151 - accuracy: 0.6934\n",
      "Epoch 1: val_accuracy improved from -inf to 0.59353, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75/assets\n",
      "901/901 [==============================] - 61s 66ms/step - loss: 0.9151 - accuracy: 0.6934 - val_loss: 1.6686 - val_accuracy: 0.5935\n",
      "Epoch 2/100\n",
      "900/901 [============================>.] - ETA: 0s - loss: 0.4951 - accuracy: 0.8310\n",
      "Epoch 2: val_accuracy did not improve from 0.59353\n",
      "901/901 [==============================] - 57s 64ms/step - loss: 0.4951 - accuracy: 0.8311 - val_loss: 2.0393 - val_accuracy: 0.5468\n",
      "Epoch 3/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.8704\n",
      "Epoch 3: val_accuracy improved from 0.59353 to 0.60861, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75/assets\n",
      "901/901 [==============================] - 58s 65ms/step - loss: 0.3769 - accuracy: 0.8704 - val_loss: 1.8662 - val_accuracy: 0.6086\n",
      "Epoch 4/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.8880\n",
      "Epoch 4: val_accuracy did not improve from 0.60861\n",
      "901/901 [==============================] - 53s 59ms/step - loss: 0.3197 - accuracy: 0.8880 - val_loss: 2.9966 - val_accuracy: 0.4952\n",
      "Epoch 5/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9026\n",
      "Epoch 5: val_accuracy did not improve from 0.60861\n",
      "901/901 [==============================] - 53s 59ms/step - loss: 0.2820 - accuracy: 0.9026 - val_loss: 2.2241 - val_accuracy: 0.5823\n",
      "Epoch 6/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.9121\n",
      "Epoch 6: val_accuracy did not improve from 0.60861\n",
      "901/901 [==============================] - 55s 61ms/step - loss: 0.2483 - accuracy: 0.9121 - val_loss: 2.5596 - val_accuracy: 0.5265\n",
      "Epoch 7/100\n",
      "900/901 [============================>.] - ETA: 0s - loss: 0.2246 - accuracy: 0.9198\n",
      "Epoch 7: val_accuracy did not improve from 0.60861\n",
      "901/901 [==============================] - 55s 61ms/step - loss: 0.2246 - accuracy: 0.9198 - val_loss: 2.8797 - val_accuracy: 0.5783\n",
      "Epoch 8/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9262\n",
      "Epoch 8: val_accuracy improved from 0.60861 to 0.63161, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75/assets\n",
      "901/901 [==============================] - 55s 61ms/step - loss: 0.2087 - accuracy: 0.9262 - val_loss: 2.6993 - val_accuracy: 0.6316\n",
      "Epoch 9/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9305\n",
      "Epoch 9: val_accuracy did not improve from 0.63161\n",
      "901/901 [==============================] - 53s 59ms/step - loss: 0.1942 - accuracy: 0.9305 - val_loss: 3.2791 - val_accuracy: 0.5230\n",
      "Epoch 10/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.9356\n",
      "Epoch 10: val_accuracy did not improve from 0.63161\n",
      "901/901 [==============================] - 52s 58ms/step - loss: 0.1778 - accuracy: 0.9356 - val_loss: 2.5975 - val_accuracy: 0.6275\n",
      "Epoch 11/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9395\n",
      "Epoch 11: val_accuracy did not improve from 0.63161\n",
      "901/901 [==============================] - 53s 58ms/step - loss: 0.1671 - accuracy: 0.9395 - val_loss: 2.4003 - val_accuracy: 0.6237\n",
      "Epoch 12/100\n",
      "341/901 [==========>...................] - ETA: 29s - loss: 0.1573 - accuracy: 0.9439"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-10456304d79f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd9906",
   "metadata": {
    "id": "92dd9906"
   },
   "outputs": [],
   "source": [
    "# load the model with the best weights\n",
    "model = load_model(MODEL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db16ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24db16ea",
    "outputId": "c3c580ae-57d0-4208-e814-f76675c10455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/101 [==============================] - 5s 43ms/step - loss: 3.3545 - accuracy: 0.6358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.3545119762420654, 0.6358116865158081]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(validation_generator)\n",
    "y_pred\n",
    "score = model.evaluate(validation_generator, verbose=1)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QzhzV-pbjKSu",
   "metadata": {
    "id": "QzhzV-pbjKSu"
   },
   "outputs": [],
   "source": [
    "with (FOLD_DATA_DIR / \"1\").open(\"br\") as f:\n",
    "    x, y, cl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5d9f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ea5d9f0",
    "outputId": "a2b75749-94d0-47b9-9219-57bb57683d78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.706875753920386, 0.706875753920386)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred_agg = sum_rule_agg(y_pred, fold_chunk_lens[9])\n",
    "y_pred_agg = sum_rule_agg(y_pred, cl)\n",
    "# y_test_agg = sum_rule_agg(validation_generator.classes)\n",
    "y_test_agg = sum_rule_agg(\n",
    "    #to_categorical(validation_generator.classes, num_classes=NUM_CLASSES), fold_chunk_lens[9]\n",
    "    to_categorical(validation_generator.classes, num_classes=NUM_CLASSES), cl\n",
    ")\n",
    "(y_pred_agg == y_test_agg).sum() / len(y_pred_agg), accuracy_score(\n",
    "    y_test_agg, y_pred_agg\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38b231",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c38b231",
    "outputId": "24931c32-0b3a-4499-8d5f-90859af068fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=int32, numpy=\n",
       "array([[61,  0,  9,  0,  0,  2,  0, 10,  1, 20],\n",
       "       [ 0, 15,  0,  0,  0,  0,  0,  0,  0,  4],\n",
       "       [ 0,  0, 83,  5,  0,  7,  3,  1,  2,  8],\n",
       "       [ 0,  0, 17, 49,  1,  1,  0,  0,  0,  6],\n",
       "       [ 0,  0,  3,  4, 75,  0,  0,  1,  0, 17],\n",
       "       [22,  0, 13,  0,  0, 54,  0,  3,  0, 12],\n",
       "       [ 0,  0,  0,  0,  0,  0,  8,  0,  0,  0],\n",
       "       [ 0,  0,  2,  0,  2,  0,  0, 44,  0, 51],\n",
       "       [ 3,  0,  9, 12,  1,  1,  0,  0, 53,  1],\n",
       "       [ 1,  0,  7,  0,  0,  0,  0,  0,  0, 99]], dtype=int32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_agg, y_pred_agg, num_classes=NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99e0f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e99e0f2",
    "outputId": "1849a512-c3c2-4ad4-e2c5-871e017d08d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop with val_idx=7\n",
      "Found 90536 images belonging to 10 classes.\n",
      "Found 9106 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.8791 - accuracy: 0.7059\n",
      "Epoch 1: val_accuracy improved from -inf to 0.60015, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi7\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi7/assets\n",
      "906/906 [==============================] - 74s 70ms/step - loss: 0.8791 - accuracy: 0.7059 - val_loss: 1.7262 - val_accuracy: 0.6002\n",
      "Epoch 2/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.8387\n",
      "Epoch 2: val_accuracy did not improve from 0.60015\n",
      "906/906 [==============================] - 60s 67ms/step - loss: 0.4747 - accuracy: 0.8387 - val_loss: 4.3563 - val_accuracy: 0.4441\n",
      "Epoch 3/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.3616 - accuracy: 0.8761\n",
      "Epoch 3: val_accuracy did not improve from 0.60015\n",
      "906/906 [==============================] - 59s 65ms/step - loss: 0.3616 - accuracy: 0.8761 - val_loss: 3.4638 - val_accuracy: 0.3956\n",
      "Epoch 4/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.8956\n",
      "Epoch 4: val_accuracy did not improve from 0.60015\n",
      "906/906 [==============================] - 61s 68ms/step - loss: 0.3019 - accuracy: 0.8956 - val_loss: 2.9549 - val_accuracy: 0.5651\n",
      "Epoch 5/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9076\n",
      "Epoch 5: val_accuracy improved from 0.60015 to 0.66099, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi7\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi7/assets\n",
      "906/906 [==============================] - 62s 69ms/step - loss: 0.2653 - accuracy: 0.9076 - val_loss: 2.2945 - val_accuracy: 0.6610\n",
      "Epoch 6/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9181\n",
      "Epoch 6: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 61s 67ms/step - loss: 0.2316 - accuracy: 0.9181 - val_loss: 3.8715 - val_accuracy: 0.4436\n",
      "Epoch 7/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9239\n",
      "Epoch 7: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 61s 67ms/step - loss: 0.2149 - accuracy: 0.9239 - val_loss: 3.3948 - val_accuracy: 0.5500\n",
      "Epoch 8/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1966 - accuracy: 0.9301\n",
      "Epoch 8: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 60s 67ms/step - loss: 0.1966 - accuracy: 0.9301 - val_loss: 3.9763 - val_accuracy: 0.5562\n",
      "Epoch 9/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.9352\n",
      "Epoch 9: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 60s 67ms/step - loss: 0.1839 - accuracy: 0.9352 - val_loss: 3.3766 - val_accuracy: 0.6047\n",
      "Epoch 10/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9401\n",
      "Epoch 10: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 60s 66ms/step - loss: 0.1692 - accuracy: 0.9401 - val_loss: 3.9314 - val_accuracy: 0.5247\n",
      "Epoch 11/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1609 - accuracy: 0.9427\n",
      "Epoch 11: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 60s 66ms/step - loss: 0.1609 - accuracy: 0.9427 - val_loss: 3.9757 - val_accuracy: 0.5793\n",
      "Epoch 12/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1510 - accuracy: 0.9455\n",
      "Epoch 12: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 60s 66ms/step - loss: 0.1510 - accuracy: 0.9455 - val_loss: 3.3754 - val_accuracy: 0.6478\n",
      "Epoch 13/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9467\n",
      "Epoch 13: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 60s 67ms/step - loss: 0.1473 - accuracy: 0.9467 - val_loss: 4.1416 - val_accuracy: 0.6103\n",
      "Epoch 14/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9501\n",
      "Epoch 14: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 60s 66ms/step - loss: 0.1392 - accuracy: 0.9501 - val_loss: 3.8985 - val_accuracy: 0.6262\n",
      "Epoch 15/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9554\n",
      "Epoch 15: val_accuracy did not improve from 0.66099\n",
      "906/906 [==============================] - 60s 66ms/step - loss: 0.1280 - accuracy: 0.9554 - val_loss: 3.5549 - val_accuracy: 0.6461\n",
      "Epoch 15: early stopping\n",
      "accuracy with val_idx=7 is 0.7406914893617021\n",
      "Starting loop with val_idx=8\n",
      "Found 90215 images belonging to 10 classes.\n",
      "Found 9427 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.9561 - accuracy: 0.6801\n",
      "Epoch 1: val_accuracy improved from -inf to 0.66840, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi8\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi8/assets\n",
      "903/903 [==============================] - 64s 70ms/step - loss: 0.9561 - accuracy: 0.6801 - val_loss: 1.2760 - val_accuracy: 0.6684\n",
      "Epoch 2/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.5192 - accuracy: 0.8253\n",
      "Epoch 2: val_accuracy improved from 0.66840 to 0.67445, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi8\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi8/assets\n",
      "903/903 [==============================] - 63s 70ms/step - loss: 0.5192 - accuracy: 0.8253 - val_loss: 1.9915 - val_accuracy: 0.6744\n",
      "Epoch 3/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.4041 - accuracy: 0.8619\n",
      "Epoch 3: val_accuracy did not improve from 0.67445\n",
      "903/903 [==============================] - 59s 66ms/step - loss: 0.4041 - accuracy: 0.8619 - val_loss: 1.9279 - val_accuracy: 0.6738\n",
      "Epoch 4/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.8814\n",
      "Epoch 4: val_accuracy did not improve from 0.67445\n",
      "903/903 [==============================] - 60s 67ms/step - loss: 0.3426 - accuracy: 0.8814 - val_loss: 2.4107 - val_accuracy: 0.6701\n",
      "Epoch 5/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.8963\n",
      "Epoch 5: val_accuracy did not improve from 0.67445\n",
      "903/903 [==============================] - 60s 66ms/step - loss: 0.2986 - accuracy: 0.8963 - val_loss: 2.9123 - val_accuracy: 0.6023\n",
      "Epoch 6/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9021\n",
      "Epoch 6: val_accuracy did not improve from 0.67445\n",
      "903/903 [==============================] - 60s 66ms/step - loss: 0.2755 - accuracy: 0.9021 - val_loss: 2.7256 - val_accuracy: 0.6484\n",
      "Epoch 7/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.9122\n",
      "Epoch 7: val_accuracy improved from 0.67445 to 0.67508, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi8\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi8/assets\n",
      "903/903 [==============================] - 62s 69ms/step - loss: 0.2500 - accuracy: 0.9122 - val_loss: 2.7922 - val_accuracy: 0.6751\n",
      "Epoch 8/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.9192\n",
      "Epoch 8: val_accuracy improved from 0.67508 to 0.69800, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi8\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi8/assets\n",
      "903/903 [==============================] - 63s 69ms/step - loss: 0.2294 - accuracy: 0.9192 - val_loss: 2.7402 - val_accuracy: 0.6980\n",
      "Epoch 9/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.2085 - accuracy: 0.9261\n",
      "Epoch 9: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 60s 66ms/step - loss: 0.2085 - accuracy: 0.9261 - val_loss: 2.8190 - val_accuracy: 0.6862\n",
      "Epoch 10/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1966 - accuracy: 0.9299\n",
      "Epoch 10: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 60s 67ms/step - loss: 0.1966 - accuracy: 0.9299 - val_loss: 2.6832 - val_accuracy: 0.6918\n",
      "Epoch 11/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.9327\n",
      "Epoch 11: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 61s 68ms/step - loss: 0.1878 - accuracy: 0.9327 - val_loss: 2.9880 - val_accuracy: 0.6968\n",
      "Epoch 12/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9369\n",
      "Epoch 12: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 61s 68ms/step - loss: 0.1767 - accuracy: 0.9369 - val_loss: 3.1578 - val_accuracy: 0.6947\n",
      "Epoch 13/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9418\n",
      "Epoch 13: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 61s 68ms/step - loss: 0.1625 - accuracy: 0.9418 - val_loss: 3.1858 - val_accuracy: 0.6917\n",
      "Epoch 14/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9433\n",
      "Epoch 14: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 61s 67ms/step - loss: 0.1600 - accuracy: 0.9433 - val_loss: 3.4415 - val_accuracy: 0.6834\n",
      "Epoch 15/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1495 - accuracy: 0.9466\n",
      "Epoch 15: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 61s 67ms/step - loss: 0.1495 - accuracy: 0.9466 - val_loss: 3.5217 - val_accuracy: 0.6526\n",
      "Epoch 16/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9483\n",
      "Epoch 16: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 61s 67ms/step - loss: 0.1473 - accuracy: 0.9483 - val_loss: 3.9284 - val_accuracy: 0.6748\n",
      "Epoch 17/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9496\n",
      "Epoch 17: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 61s 67ms/step - loss: 0.1370 - accuracy: 0.9496 - val_loss: 3.7954 - val_accuracy: 0.6673\n",
      "Epoch 18/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9521\n",
      "Epoch 18: val_accuracy did not improve from 0.69800\n",
      "903/903 [==============================] - 60s 66ms/step - loss: 0.1326 - accuracy: 0.9521 - val_loss: 3.8958 - val_accuracy: 0.6875\n",
      "Epoch 18: early stopping\n",
      "accuracy with val_idx=8 is 0.7461538461538462\n",
      "Starting loop with val_idx=9\n",
      "Found 90030 images belonging to 10 classes.\n",
      "Found 9612 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.9481 - accuracy: 0.6838\n",
      "Epoch 1: val_accuracy improved from -inf to 0.60903, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi9\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi9/assets\n",
      "901/901 [==============================] - 65s 71ms/step - loss: 0.9481 - accuracy: 0.6838 - val_loss: 1.2653 - val_accuracy: 0.6090\n",
      "Epoch 2/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.8293\n",
      "Epoch 2: val_accuracy did not improve from 0.60903\n",
      "901/901 [==============================] - 60s 67ms/step - loss: 0.5034 - accuracy: 0.8293 - val_loss: 2.3615 - val_accuracy: 0.5027\n",
      "Epoch 3/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.8695\n",
      "Epoch 3: val_accuracy did not improve from 0.60903\n",
      "901/901 [==============================] - 60s 67ms/step - loss: 0.3798 - accuracy: 0.8695 - val_loss: 1.5723 - val_accuracy: 0.5886\n",
      "Epoch 4/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.8913\n",
      "Epoch 4: val_accuracy did not improve from 0.60903\n",
      "901/901 [==============================] - 60s 67ms/step - loss: 0.3126 - accuracy: 0.8913 - val_loss: 2.6917 - val_accuracy: 0.5726\n",
      "Epoch 5/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2727 - accuracy: 0.9052\n",
      "Epoch 5: val_accuracy improved from 0.60903 to 0.60934, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi9\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi9/assets\n",
      "901/901 [==============================] - 63s 70ms/step - loss: 0.2727 - accuracy: 0.9052 - val_loss: 1.8229 - val_accuracy: 0.6093\n",
      "Epoch 6/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9162\n",
      "Epoch 6: val_accuracy did not improve from 0.60934\n",
      "901/901 [==============================] - 60s 67ms/step - loss: 0.2421 - accuracy: 0.9162 - val_loss: 2.8017 - val_accuracy: 0.5539\n",
      "Epoch 7/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9227\n",
      "Epoch 7: val_accuracy did not improve from 0.60934\n",
      "901/901 [==============================] - 60s 66ms/step - loss: 0.2228 - accuracy: 0.9227 - val_loss: 3.7419 - val_accuracy: 0.4877\n",
      "Epoch 8/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.9295\n",
      "Epoch 8: val_accuracy did not improve from 0.60934\n",
      "901/901 [==============================] - 60s 66ms/step - loss: 0.1993 - accuracy: 0.9295 - val_loss: 2.2510 - val_accuracy: 0.5956\n",
      "Epoch 9/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9348\n",
      "Epoch 9: val_accuracy did not improve from 0.60934\n",
      "901/901 [==============================] - 60s 66ms/step - loss: 0.1868 - accuracy: 0.9348 - val_loss: 2.5990 - val_accuracy: 0.5799\n",
      "Epoch 10/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9379\n",
      "Epoch 10: val_accuracy did not improve from 0.60934\n",
      "901/901 [==============================] - 60s 66ms/step - loss: 0.1729 - accuracy: 0.9379 - val_loss: 2.9603 - val_accuracy: 0.5834\n",
      "Epoch 11/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9406\n",
      "Epoch 11: val_accuracy did not improve from 0.60934\n",
      "901/901 [==============================] - 60s 67ms/step - loss: 0.1670 - accuracy: 0.9406 - val_loss: 2.1900 - val_accuracy: 0.5779\n",
      "Epoch 12/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1528 - accuracy: 0.9453\n",
      "Epoch 12: val_accuracy did not improve from 0.60934\n",
      "901/901 [==============================] - 60s 67ms/step - loss: 0.1528 - accuracy: 0.9453 - val_loss: 2.5267 - val_accuracy: 0.5659\n",
      "Epoch 13/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.9474\n",
      "Epoch 13: val_accuracy improved from 0.60934 to 0.62256, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi9\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi9/assets\n",
      "901/901 [==============================] - 62s 69ms/step - loss: 0.1489 - accuracy: 0.9474 - val_loss: 2.2924 - val_accuracy: 0.6226\n",
      "Epoch 14/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9499\n",
      "Epoch 14: val_accuracy did not improve from 0.62256\n",
      "901/901 [==============================] - 59s 66ms/step - loss: 0.1391 - accuracy: 0.9499 - val_loss: 2.2338 - val_accuracy: 0.6215\n",
      "Epoch 15/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9526\n",
      "Epoch 15: val_accuracy did not improve from 0.62256\n",
      "901/901 [==============================] - 60s 66ms/step - loss: 0.1307 - accuracy: 0.9526 - val_loss: 2.6788 - val_accuracy: 0.5756\n",
      "Epoch 16/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9554\n",
      "Epoch 16: val_accuracy improved from 0.62256 to 0.64992, saving model to /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi9\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/data/2d_saved_models/model.Sr16000Cs16000Ol75Vi9/assets\n",
      "901/901 [==============================] - 62s 69ms/step - loss: 0.1252 - accuracy: 0.9554 - val_loss: 2.1414 - val_accuracy: 0.6499\n",
      "Epoch 17/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9559\n",
      "Epoch 17: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 61s 67ms/step - loss: 0.1227 - accuracy: 0.9559 - val_loss: 3.0686 - val_accuracy: 0.6248\n",
      "Epoch 18/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9578\n",
      "Epoch 18: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 61s 68ms/step - loss: 0.1184 - accuracy: 0.9578 - val_loss: 4.2489 - val_accuracy: 0.5746\n",
      "Epoch 19/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9595\n",
      "Epoch 19: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 60s 67ms/step - loss: 0.1107 - accuracy: 0.9595 - val_loss: 3.5854 - val_accuracy: 0.5419\n",
      "Epoch 20/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9598\n",
      "Epoch 20: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 61s 67ms/step - loss: 0.1089 - accuracy: 0.9598 - val_loss: 3.0432 - val_accuracy: 0.6178\n",
      "Epoch 21/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9613\n",
      "Epoch 21: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 60s 66ms/step - loss: 0.1070 - accuracy: 0.9613 - val_loss: 2.9309 - val_accuracy: 0.6085\n",
      "Epoch 22/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9621\n",
      "Epoch 22: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 60s 67ms/step - loss: 0.1047 - accuracy: 0.9621 - val_loss: 3.3240 - val_accuracy: 0.6066\n",
      "Epoch 23/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9644\n",
      "Epoch 23: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 59s 66ms/step - loss: 0.1009 - accuracy: 0.9644 - val_loss: 2.7933 - val_accuracy: 0.6392\n",
      "Epoch 24/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9644\n",
      "Epoch 24: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 59s 66ms/step - loss: 0.0979 - accuracy: 0.9644 - val_loss: 2.7802 - val_accuracy: 0.6197\n",
      "Epoch 25/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9666\n",
      "Epoch 25: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 58s 65ms/step - loss: 0.0918 - accuracy: 0.9666 - val_loss: 2.2176 - val_accuracy: 0.6388\n",
      "Epoch 26/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9668\n",
      "Epoch 26: val_accuracy did not improve from 0.64992\n",
      "901/901 [==============================] - 59s 66ms/step - loss: 0.0896 - accuracy: 0.9668 - val_loss: 7.3881 - val_accuracy: 0.4880\n",
      "Epoch 26: early stopping\n",
      "accuracy with val_idx=9 is 0.7110834371108343\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "accuracies = []\n",
    "confusion_matrices = []\n",
    "for val_idx in range(7, 10):\n",
    "    print(f\"Starting loop with val_idx={val_idx}\")\n",
    "\n",
    "    copy_to_train_test_dir(test_fold=val_idx+1)\n",
    "    model = build_model(input_shape=(72, 72, 3))\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "    val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR, target_size=(72, 72), batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        TEST_DIR,\n",
    "        #target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        #target_size=(224, 224),\n",
    "        target_size=(72, 72),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    cpcallback = ModelCheckpoint(\n",
    "        monitor=\"val_accuracy\",\n",
    "        filepath=f\"{MODEL_FILE}Vi{val_idx}\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks[0] = cpcallback\n",
    "    hist = model.fit(\n",
    "        train_generator,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # load the model with the best weights\n",
    "    model = load_model(f\"{MODEL_FILE}Vi{val_idx}\")\n",
    "\n",
    "    y_pred = model.predict(validation_generator)\n",
    "\n",
    "    with (FOLD_DATA_DIR / f\"{val_idx+1}\").open(\"br\") as f:\n",
    "        x, y, cl = pickle.load(f)\n",
    "\n",
    "    y_pred_agg = sum_rule_agg(y_pred, cl)\n",
    "    y_test_agg = sum_rule_agg(to_categorical(validation_generator.classes, num_classes=NUM_CLASSES), cl)\n",
    "\n",
    "    acc = (y_pred_agg == y_test_agg).sum() / len(y_pred_agg)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"accuracy with val_idx={val_idx} is {acc}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test_agg, y_pred_agg, num_classes=NUM_CLASSES)\n",
    "    confusion_matrices.append(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b5cb25",
   "metadata": {
    "id": "20b5cb25"
   },
   "outputs": [],
   "source": [
    "#accuracies, confusion_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "190f3e9f-3acc-476d-8f1d-91b875f67a05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "190f3e9f-3acc-476d-8f1d-91b875f67a05",
    "outputId": "5a07773d-1287-43b6-f160-b357bba88043"
   },
   "outputs": [],
   "source": [
    "# VGG16 training\n",
    "\n",
    "\n",
    "def build_vgg_model(input_shape):\n",
    "    # include_top=false means ignore the FC classifier part on top of the model\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False,\n",
    "                      input_shape=(72, 72, 3),\n",
    "                      classes=NUM_CLASSES)\n",
    "\n",
    "    # base_model = tf.keras.applications.resnet50.ResNet50(\n",
    "    #     include_top=False,\n",
    "    #     weights='imagenet',\n",
    "    #     input_shape=(72, 72, 3),\n",
    "    #     classes=10\n",
    "    # )\n",
    "    # base_model.summary()\n",
    "\n",
    "    # freeze all layers except last CONV layer\n",
    "    base_model.trainable = False\n",
    "    base_model.layers[-2].trainable = True\n",
    "\n",
    "    # add 1 Dense layer with 64 neurons\n",
    "    x = Flatten()(base_model.layers[-1].output)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    new_model = Model(inputs=base_model.input, outputs=x)\n",
    "    new_model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95745925-5ff7-45c6-8f49-606ec07189f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = build_vgg_model((72, 72, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "up8Ff53opNCd",
   "metadata": {
    "id": "up8Ff53opNCd"
   },
   "outputs": [],
   "source": [
    "copy_to_train_test_dir(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "njBOvaNQOK9E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njBOvaNQOK9E",
    "outputId": "c85979e4-0dc2-4f8e-8397-ae875fbb4113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90030 images belonging to 10 classes.\n",
      "Found 9612 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR, target_size=(72, 72), batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(72, 72),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "oWX6oi0kOV_y",
   "metadata": {
    "id": "oWX6oi0kOV_y"
   },
   "outputs": [],
   "source": [
    "# callbacks to save, stop early and visualize\n",
    "cpcallback = ModelCheckpoint(\n",
    "    monitor=\"val_accuracy\", filepath=VGG_MODEL_FILE, save_best_only=True, verbose=1\n",
    ")\n",
    "escallback = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", min_delta=0, patience=5, verbose=1\n",
    ")\n",
    "tbcallback = TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "callbacks = [cpcallback, escallback, tbcallback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "T9zZAoy2ohtk",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9zZAoy2ohtk",
    "outputId": "97d24b2a-e556-4344-c8ef-6b74052c6e5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.6160 - accuracy: 0.7928\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.70745, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75/assets\n",
      "901/901 [==============================] - 41s 45ms/step - loss: 0.6160 - accuracy: 0.7928 - val_loss: 0.9211 - val_accuracy: 0.7074\n",
      "Epoch 2/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.8843\n",
      "Epoch 00002: val_accuracy improved from 0.70745 to 0.71119, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75/assets\n",
      "901/901 [==============================] - 40s 44ms/step - loss: 0.3425 - accuracy: 0.8843 - val_loss: 1.0047 - val_accuracy: 0.7112\n",
      "Epoch 3/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9090\n",
      "Epoch 00003: val_accuracy improved from 0.71119 to 0.72722, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75/assets\n",
      "901/901 [==============================] - 40s 44ms/step - loss: 0.2645 - accuracy: 0.9090 - val_loss: 1.1073 - val_accuracy: 0.7272\n",
      "Epoch 4/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2177 - accuracy: 0.9251\n",
      "Epoch 00004: val_accuracy improved from 0.72722 to 0.74875, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75/assets\n",
      "901/901 [==============================] - 40s 44ms/step - loss: 0.2177 - accuracy: 0.9251 - val_loss: 1.0882 - val_accuracy: 0.7488\n",
      "Epoch 5/100\n",
      "900/901 [============================>.] - ETA: 0s - loss: 0.1786 - accuracy: 0.9371\n",
      "Epoch 00005: val_accuracy did not improve from 0.74875\n",
      "901/901 [==============================] - 40s 44ms/step - loss: 0.1786 - accuracy: 0.9371 - val_loss: 1.1459 - val_accuracy: 0.7327\n",
      "Epoch 6/100\n",
      "900/901 [============================>.] - ETA: 0s - loss: 0.1620 - accuracy: 0.9429\n",
      "Epoch 00006: val_accuracy did not improve from 0.74875\n",
      "901/901 [==============================] - 37s 41ms/step - loss: 0.1619 - accuracy: 0.9429 - val_loss: 1.2339 - val_accuracy: 0.7311\n",
      "Epoch 7/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9507\n",
      "Epoch 00007: val_accuracy did not improve from 0.74875\n",
      "901/901 [==============================] - 37s 42ms/step - loss: 0.1397 - accuracy: 0.9507 - val_loss: 1.3938 - val_accuracy: 0.7346\n",
      "Epoch 8/100\n",
      "900/901 [============================>.] - ETA: 0s - loss: 0.1280 - accuracy: 0.9554\n",
      "Epoch 00008: val_accuracy did not improve from 0.74875\n",
      "901/901 [==============================] - 37s 41ms/step - loss: 0.1280 - accuracy: 0.9554 - val_loss: 1.4847 - val_accuracy: 0.7302\n",
      "Epoch 9/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9596\n",
      "Epoch 00009: val_accuracy did not improve from 0.74875\n",
      "901/901 [==============================] - 37s 41ms/step - loss: 0.1149 - accuracy: 0.9596 - val_loss: 1.6019 - val_accuracy: 0.7250\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = new_model.fit(\n",
    "    train_generator,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c331a0b-425d-4755-8f09-2b17f0098625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with val_idx=9 is 0.8076463560334528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2829/833608371.py:3: RuntimeWarning: Mean of empty slice.\n",
      "  [res.mean(axis=0).argmax() for res in np.split(y, chunk_lens.cumsum()[:-1])]\n",
      "/data/arpank/miniforge3/envs/dlproject/lib/python3.10/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=int32, numpy=\n",
       "array([[ 82,   0,   7,   5,   0,   0,   0,   7,   5,   4],\n",
       "       [  1,  14,   0,   0,   0,   0,   0,   0,   0,   3],\n",
       "       [  0,   0, 100,   6,   4,   0,   1,   0,   1,   0],\n",
       "       [  0,   0,  10,  60,   0,   0,   0,   1,   0,   6],\n",
       "       [  2,   0,   1,   2,  86,   0,   0,   2,   1,  10],\n",
       "       [  8,   0,   8,   0,   1,  78,   0,   5,  10,   1],\n",
       "       [  0,   0,   0,   0,   0,   0,   8,   1,   0,   0],\n",
       "       [  0,   0,   0,   1,   1,   1,   0, 102,   0,   0],\n",
       "       [  0,   1,   7,  14,   0,   0,   0,   1,  56,   1],\n",
       "       [  0,   1,  16,   0,   0,   0,   0,   3,   1,  90]], dtype=int32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model with the best weights\n",
    "model = load_model(f\"{VGG_MODEL_FILE}\")\n",
    "\n",
    "y_pred = model.predict(validation_generator)\n",
    "\n",
    "#     with (FOLD_DATA_DIR / f\"{val_idx+1}\").open(\"br\") as f:\n",
    "#         x, y, cl = pickle.load(f)\n",
    "val_idx = 9\n",
    "cl = fold_chunk_lens[val_idx]\n",
    "\n",
    "y_pred_agg = sum_rule_agg(y_pred, cl)\n",
    "y_test_agg = sum_rule_agg(to_categorical(validation_generator.classes, num_classes=NUM_CLASSES), cl)\n",
    "\n",
    "acc = (y_pred_agg == y_test_agg).sum() / len(y_pred_agg)\n",
    "print(f\"accuracy with val_idx={val_idx} is {acc}\")\n",
    "\n",
    "cm = confusion_matrix(y_test_agg, y_pred_agg, num_classes=NUM_CLASSES)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "Alt5x5t6pk92",
   "metadata": {
    "id": "Alt5x5t6pk92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop with val_idx=1\n",
      "Found 89706 images belonging to 10 classes.\n",
      "Found 9936 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "897/898 [============================>.] - ETA: 0s - loss: 0.6181 - accuracy: 0.7910\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.66425, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi1\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi1/assets\n",
      "898/898 [==============================] - 44s 48ms/step - loss: 0.6180 - accuracy: 0.7911 - val_loss: 1.1619 - val_accuracy: 0.6643\n",
      "Epoch 2/100\n",
      "897/898 [============================>.] - ETA: 0s - loss: 0.3547 - accuracy: 0.8787\n",
      "Epoch 00002: val_accuracy did not improve from 0.66425\n",
      "898/898 [==============================] - 40s 44ms/step - loss: 0.3547 - accuracy: 0.8787 - val_loss: 1.1854 - val_accuracy: 0.6469\n",
      "Epoch 3/100\n",
      "898/898 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.9087\n",
      "Epoch 00003: val_accuracy did not improve from 0.66425\n",
      "898/898 [==============================] - 39s 43ms/step - loss: 0.2664 - accuracy: 0.9087 - val_loss: 1.4128 - val_accuracy: 0.6216\n",
      "Epoch 4/100\n",
      "898/898 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 0.9246\n",
      "Epoch 00004: val_accuracy did not improve from 0.66425\n",
      "898/898 [==============================] - 39s 43ms/step - loss: 0.2179 - accuracy: 0.9246 - val_loss: 1.4971 - val_accuracy: 0.6206\n",
      "Epoch 5/100\n",
      "897/898 [============================>.] - ETA: 0s - loss: 0.1887 - accuracy: 0.9354\n",
      "Epoch 00005: val_accuracy did not improve from 0.66425\n",
      "898/898 [==============================] - 38s 43ms/step - loss: 0.1886 - accuracy: 0.9354 - val_loss: 1.5680 - val_accuracy: 0.6607\n",
      "Epoch 6/100\n",
      "898/898 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9435\n",
      "Epoch 00006: val_accuracy did not improve from 0.66425\n",
      "898/898 [==============================] - 39s 43ms/step - loss: 0.1634 - accuracy: 0.9435 - val_loss: 1.7964 - val_accuracy: 0.6414\n",
      "Epoch 00006: early stopping\n",
      "accuracy with val_idx=1 is 0.748780487804878\n",
      "Starting loop with val_idx=2\n",
      "Found 88833 images belonging to 10 classes.\n",
      "Found 10809 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "888/889 [============================>.] - ETA: 0s - loss: 0.6018 - accuracy: 0.7976\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.57720, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi2\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi2/assets\n",
      "889/889 [==============================] - 43s 48ms/step - loss: 0.6015 - accuracy: 0.7977 - val_loss: 1.3778 - val_accuracy: 0.5772\n",
      "Epoch 2/100\n",
      "889/889 [==============================] - ETA: 0s - loss: 0.3337 - accuracy: 0.8862\n",
      "Epoch 00002: val_accuracy did not improve from 0.57720\n",
      "889/889 [==============================] - 40s 45ms/step - loss: 0.3337 - accuracy: 0.8862 - val_loss: 1.6489 - val_accuracy: 0.5475\n",
      "Epoch 3/100\n",
      "889/889 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9109\n",
      "Epoch 00003: val_accuracy did not improve from 0.57720\n",
      "889/889 [==============================] - 39s 44ms/step - loss: 0.2560 - accuracy: 0.9109 - val_loss: 2.0856 - val_accuracy: 0.5173\n",
      "Epoch 4/100\n",
      "888/889 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9274\n",
      "Epoch 00004: val_accuracy did not improve from 0.57720\n",
      "889/889 [==============================] - 39s 44ms/step - loss: 0.2097 - accuracy: 0.9274 - val_loss: 2.1387 - val_accuracy: 0.5148\n",
      "Epoch 5/100\n",
      "888/889 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9391\n",
      "Epoch 00005: val_accuracy did not improve from 0.57720\n",
      "889/889 [==============================] - 39s 44ms/step - loss: 0.1765 - accuracy: 0.9392 - val_loss: 2.7388 - val_accuracy: 0.5251\n",
      "Epoch 6/100\n",
      "889/889 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9448\n",
      "Epoch 00006: val_accuracy did not improve from 0.57720\n",
      "889/889 [==============================] - 39s 44ms/step - loss: 0.1569 - accuracy: 0.9448 - val_loss: 2.3827 - val_accuracy: 0.5315\n",
      "Epoch 00006: early stopping\n",
      "accuracy with val_idx=2 is 0.6429378531073446\n",
      "Starting loop with val_idx=3\n",
      "Found 88511 images belonging to 10 classes.\n",
      "Found 11131 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "885/886 [============================>.] - ETA: 0s - loss: 0.5976 - accuracy: 0.8000\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.63615, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi3\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi3/assets\n",
      "886/886 [==============================] - 44s 49ms/step - loss: 0.5974 - accuracy: 0.8000 - val_loss: 1.3633 - val_accuracy: 0.6362\n",
      "Epoch 2/100\n",
      "885/886 [============================>.] - ETA: 0s - loss: 0.3423 - accuracy: 0.8833\n",
      "Epoch 00002: val_accuracy improved from 0.63615 to 0.65205, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi3\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi3/assets\n",
      "886/886 [==============================] - 41s 47ms/step - loss: 0.3421 - accuracy: 0.8833 - val_loss: 1.2989 - val_accuracy: 0.6521\n",
      "Epoch 3/100\n",
      "885/886 [============================>.] - ETA: 0s - loss: 0.2571 - accuracy: 0.9113\n",
      "Epoch 00003: val_accuracy did not improve from 0.65205\n",
      "886/886 [==============================] - 39s 44ms/step - loss: 0.2570 - accuracy: 0.9113 - val_loss: 1.4896 - val_accuracy: 0.6451\n",
      "Epoch 4/100\n",
      "886/886 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9263\n",
      "Epoch 00004: val_accuracy did not improve from 0.65205\n",
      "886/886 [==============================] - 39s 44ms/step - loss: 0.2117 - accuracy: 0.9263 - val_loss: 1.5824 - val_accuracy: 0.6311\n",
      "Epoch 5/100\n",
      "885/886 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9369\n",
      "Epoch 00005: val_accuracy did not improve from 0.65205\n",
      "886/886 [==============================] - 41s 46ms/step - loss: 0.1807 - accuracy: 0.9369 - val_loss: 1.6111 - val_accuracy: 0.6461\n",
      "Epoch 6/100\n",
      "886/886 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9456\n",
      "Epoch 00006: val_accuracy did not improve from 0.65205\n",
      "886/886 [==============================] - 40s 45ms/step - loss: 0.1554 - accuracy: 0.9456 - val_loss: 1.8346 - val_accuracy: 0.6407\n",
      "Epoch 7/100\n",
      "886/886 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9506\n",
      "Epoch 00007: val_accuracy improved from 0.65205 to 0.65385, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi3\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi3/assets\n",
      "886/886 [==============================] - 41s 46ms/step - loss: 0.1395 - accuracy: 0.9506 - val_loss: 1.8315 - val_accuracy: 0.6538\n",
      "Epoch 8/100\n",
      "886/886 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9550\n",
      "Epoch 00008: val_accuracy did not improve from 0.65385\n",
      "886/886 [==============================] - 40s 45ms/step - loss: 0.1262 - accuracy: 0.9550 - val_loss: 1.8863 - val_accuracy: 0.6523\n",
      "Epoch 9/100\n",
      "885/886 [============================>.] - ETA: 0s - loss: 0.1102 - accuracy: 0.9611\n",
      "Epoch 00009: val_accuracy did not improve from 0.65385\n",
      "886/886 [==============================] - 39s 44ms/step - loss: 0.1101 - accuracy: 0.9611 - val_loss: 2.2367 - val_accuracy: 0.6460\n",
      "Epoch 10/100\n",
      "885/886 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9642\n",
      "Epoch 00010: val_accuracy did not improve from 0.65385\n",
      "886/886 [==============================] - 40s 45ms/step - loss: 0.1010 - accuracy: 0.9642 - val_loss: 2.1279 - val_accuracy: 0.6363\n",
      "Epoch 11/100\n",
      "885/886 [============================>.] - ETA: 0s - loss: 0.0885 - accuracy: 0.9681\n",
      "Epoch 00011: val_accuracy did not improve from 0.65385\n",
      "886/886 [==============================] - 39s 44ms/step - loss: 0.0885 - accuracy: 0.9681 - val_loss: 2.1990 - val_accuracy: 0.6309\n",
      "Epoch 12/100\n",
      "886/886 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9696\n",
      "Epoch 00012: val_accuracy did not improve from 0.65385\n",
      "886/886 [==============================] - 39s 45ms/step - loss: 0.0858 - accuracy: 0.9696 - val_loss: 2.1716 - val_accuracy: 0.6513\n",
      "Epoch 00012: early stopping\n",
      "accuracy with val_idx=3 is 0.7114164904862579\n",
      "Starting loop with val_idx=4\n",
      "Found 89174 images belonging to 10 classes.\n",
      "Found 10468 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "891/892 [============================>.] - ETA: 0s - loss: 0.6281 - accuracy: 0.7879\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.72765, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi4\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi4/assets\n",
      "892/892 [==============================] - 45s 50ms/step - loss: 0.6279 - accuracy: 0.7880 - val_loss: 0.8124 - val_accuracy: 0.7276\n",
      "Epoch 2/100\n",
      "891/892 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8802\n",
      "Epoch 00002: val_accuracy improved from 0.72765 to 0.73558, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi4\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi4/assets\n",
      "892/892 [==============================] - 40s 45ms/step - loss: 0.3506 - accuracy: 0.8802 - val_loss: 0.8588 - val_accuracy: 0.7356\n",
      "Epoch 3/100\n",
      "892/892 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.9096\n",
      "Epoch 00003: val_accuracy did not improve from 0.73558\n",
      "892/892 [==============================] - 39s 44ms/step - loss: 0.2631 - accuracy: 0.9096 - val_loss: 1.0055 - val_accuracy: 0.7129\n",
      "Epoch 4/100\n",
      "891/892 [============================>.] - ETA: 0s - loss: 0.2201 - accuracy: 0.9234\n",
      "Epoch 00004: val_accuracy did not improve from 0.73558\n",
      "892/892 [==============================] - 39s 44ms/step - loss: 0.2201 - accuracy: 0.9234 - val_loss: 1.0237 - val_accuracy: 0.7167\n",
      "Epoch 5/100\n",
      "891/892 [============================>.] - ETA: 0s - loss: 0.1833 - accuracy: 0.9364\n",
      "Epoch 00005: val_accuracy did not improve from 0.73558\n",
      "892/892 [==============================] - 40s 45ms/step - loss: 0.1833 - accuracy: 0.9364 - val_loss: 1.3599 - val_accuracy: 0.6706\n",
      "Epoch 6/100\n",
      "891/892 [============================>.] - ETA: 0s - loss: 0.1592 - accuracy: 0.9444\n",
      "Epoch 00006: val_accuracy did not improve from 0.73558\n",
      "892/892 [==============================] - 39s 44ms/step - loss: 0.1593 - accuracy: 0.9444 - val_loss: 1.4052 - val_accuracy: 0.6761\n",
      "Epoch 7/100\n",
      "892/892 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9500\n",
      "Epoch 00007: val_accuracy did not improve from 0.73558\n",
      "892/892 [==============================] - 39s 44ms/step - loss: 0.1403 - accuracy: 0.9500 - val_loss: 1.4558 - val_accuracy: 0.6878\n",
      "Epoch 00007: early stopping\n",
      "accuracy with val_idx=4 is 0.8020361990950227\n",
      "Starting loop with val_idx=5\n",
      "Found 90274 images belonging to 10 classes.\n",
      "Found 9368 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.6303 - accuracy: 0.7877\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.64859, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi5\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi5/assets\n",
      "903/903 [==============================] - 43s 47ms/step - loss: 0.6303 - accuracy: 0.7877 - val_loss: 1.3887 - val_accuracy: 0.6486\n",
      "Epoch 2/100\n",
      "902/903 [============================>.] - ETA: 0s - loss: 0.3515 - accuracy: 0.8810\n",
      "Epoch 00002: val_accuracy did not improve from 0.64859\n",
      "903/903 [==============================] - 40s 44ms/step - loss: 0.3515 - accuracy: 0.8810 - val_loss: 1.9719 - val_accuracy: 0.6355\n",
      "Epoch 3/100\n",
      "902/903 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.9095\n",
      "Epoch 00003: val_accuracy did not improve from 0.64859\n",
      "903/903 [==============================] - 39s 43ms/step - loss: 0.2655 - accuracy: 0.9096 - val_loss: 2.2045 - val_accuracy: 0.6251\n",
      "Epoch 4/100\n",
      "902/903 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.9242\n",
      "Epoch 00004: val_accuracy did not improve from 0.64859\n",
      "903/903 [==============================] - 40s 44ms/step - loss: 0.2178 - accuracy: 0.9242 - val_loss: 2.3682 - val_accuracy: 0.6214\n",
      "Epoch 5/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9360\n",
      "Epoch 00005: val_accuracy did not improve from 0.64859\n",
      "903/903 [==============================] - 40s 44ms/step - loss: 0.1815 - accuracy: 0.9360 - val_loss: 2.6817 - val_accuracy: 0.6390\n",
      "Epoch 6/100\n",
      "902/903 [============================>.] - ETA: 0s - loss: 0.1593 - accuracy: 0.9442\n",
      "Epoch 00006: val_accuracy did not improve from 0.64859\n",
      "903/903 [==============================] - 39s 43ms/step - loss: 0.1593 - accuracy: 0.9441 - val_loss: 2.8365 - val_accuracy: 0.6230\n",
      "Epoch 00006: early stopping\n",
      "accuracy with val_idx=5 is 0.7312661498708011\n",
      "Starting loop with val_idx=6\n",
      "Found 89904 images belonging to 10 classes.\n",
      "Found 9738 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.6033 - accuracy: 0.7978\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.65763, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi6\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi6/assets\n",
      "900/900 [==============================] - 43s 47ms/step - loss: 0.6033 - accuracy: 0.7978 - val_loss: 1.2318 - val_accuracy: 0.6576\n",
      "Epoch 2/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.8842\n",
      "Epoch 00002: val_accuracy did not improve from 0.65763\n",
      "900/900 [==============================] - 39s 44ms/step - loss: 0.3410 - accuracy: 0.8842 - val_loss: 1.3742 - val_accuracy: 0.6566\n",
      "Epoch 3/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9088\n",
      "Epoch 00003: val_accuracy did not improve from 0.65763\n",
      "900/900 [==============================] - 39s 44ms/step - loss: 0.2651 - accuracy: 0.9088 - val_loss: 1.3912 - val_accuracy: 0.6308\n",
      "Epoch 4/100\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.9278\n",
      "Epoch 00004: val_accuracy did not improve from 0.65763\n",
      "900/900 [==============================] - 39s 43ms/step - loss: 0.2108 - accuracy: 0.9278 - val_loss: 1.6333 - val_accuracy: 0.6124\n",
      "Epoch 5/100\n",
      "899/900 [============================>.] - ETA: 0s - loss: 0.1824 - accuracy: 0.9360\n",
      "Epoch 00005: val_accuracy did not improve from 0.65763\n",
      "900/900 [==============================] - 38s 43ms/step - loss: 0.1825 - accuracy: 0.9360 - val_loss: 1.6778 - val_accuracy: 0.6496\n",
      "Epoch 6/100\n",
      "899/900 [============================>.] - ETA: 0s - loss: 0.1573 - accuracy: 0.9456\n",
      "Epoch 00006: val_accuracy did not improve from 0.65763\n",
      "900/900 [==============================] - 41s 45ms/step - loss: 0.1573 - accuracy: 0.9456 - val_loss: 1.7552 - val_accuracy: 0.6575\n",
      "Epoch 00006: early stopping\n",
      "accuracy with val_idx=6 is 0.7231920199501247\n",
      "Starting loop with val_idx=7\n",
      "Found 90536 images belonging to 10 classes.\n",
      "Found 9106 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.6002 - accuracy: 0.7975\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.59126, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi7\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi7/assets\n",
      "906/906 [==============================] - 43s 47ms/step - loss: 0.6002 - accuracy: 0.7975 - val_loss: 1.6206 - val_accuracy: 0.5913\n",
      "Epoch 2/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.8854\n",
      "Epoch 00002: val_accuracy improved from 0.59126 to 0.61256, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi7\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi7/assets\n",
      "906/906 [==============================] - 41s 45ms/step - loss: 0.3382 - accuracy: 0.8854 - val_loss: 1.6841 - val_accuracy: 0.6126\n",
      "Epoch 3/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.2581 - accuracy: 0.9106\n",
      "Epoch 00003: val_accuracy improved from 0.61256 to 0.63606, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi7\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi7/assets\n",
      "906/906 [==============================] - 40s 44ms/step - loss: 0.2581 - accuracy: 0.9106 - val_loss: 1.7820 - val_accuracy: 0.6361\n",
      "Epoch 4/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.9243\n",
      "Epoch 00004: val_accuracy improved from 0.63606 to 0.66209, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi7\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi7/assets\n",
      "906/906 [==============================] - 40s 45ms/step - loss: 0.2174 - accuracy: 0.9243 - val_loss: 1.8049 - val_accuracy: 0.6621\n",
      "Epoch 5/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9379\n",
      "Epoch 00005: val_accuracy did not improve from 0.66209\n",
      "906/906 [==============================] - 39s 43ms/step - loss: 0.1774 - accuracy: 0.9379 - val_loss: 2.2060 - val_accuracy: 0.6226\n",
      "Epoch 6/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9449\n",
      "Epoch 00006: val_accuracy did not improve from 0.66209\n",
      "906/906 [==============================] - 40s 44ms/step - loss: 0.1559 - accuracy: 0.9449 - val_loss: 1.9928 - val_accuracy: 0.6189\n",
      "Epoch 7/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9518\n",
      "Epoch 00007: val_accuracy did not improve from 0.66209\n",
      "906/906 [==============================] - 39s 43ms/step - loss: 0.1383 - accuracy: 0.9518 - val_loss: 2.2974 - val_accuracy: 0.6355\n",
      "Epoch 8/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9570\n",
      "Epoch 00008: val_accuracy did not improve from 0.66209\n",
      "906/906 [==============================] - 38s 42ms/step - loss: 0.1196 - accuracy: 0.9570 - val_loss: 2.2113 - val_accuracy: 0.6453\n",
      "Epoch 9/100\n",
      "905/906 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9600\n",
      "Epoch 00009: val_accuracy did not improve from 0.66209\n",
      "906/906 [==============================] - 39s 43ms/step - loss: 0.1093 - accuracy: 0.9600 - val_loss: 2.4977 - val_accuracy: 0.6463\n",
      "Epoch 00009: early stopping\n",
      "accuracy with val_idx=7 is 0.7433510638297872\n",
      "Starting loop with val_idx=8\n",
      "Found 90215 images belonging to 10 classes.\n",
      "Found 9427 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.6172 - accuracy: 0.7934\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.70372, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi8\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi8/assets\n",
      "903/903 [==============================] - 42s 46ms/step - loss: 0.6172 - accuracy: 0.7934 - val_loss: 1.2598 - val_accuracy: 0.7037\n",
      "Epoch 2/100\n",
      "902/903 [============================>.] - ETA: 0s - loss: 0.3530 - accuracy: 0.8805\n",
      "Epoch 00002: val_accuracy improved from 0.70372 to 0.72006, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi8\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi8/assets\n",
      "903/903 [==============================] - 41s 46ms/step - loss: 0.3531 - accuracy: 0.8805 - val_loss: 1.4417 - val_accuracy: 0.7201\n",
      "Epoch 3/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.9085\n",
      "Epoch 00003: val_accuracy did not improve from 0.72006\n",
      "903/903 [==============================] - 39s 43ms/step - loss: 0.2683 - accuracy: 0.9085 - val_loss: 1.8226 - val_accuracy: 0.6930\n",
      "Epoch 4/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.2180 - accuracy: 0.9243\n",
      "Epoch 00004: val_accuracy did not improve from 0.72006\n",
      "903/903 [==============================] - 41s 45ms/step - loss: 0.2180 - accuracy: 0.9243 - val_loss: 1.9917 - val_accuracy: 0.6981\n",
      "Epoch 5/100\n",
      "903/903 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9356\n",
      "Epoch 00005: val_accuracy did not improve from 0.72006\n",
      "903/903 [==============================] - 38s 42ms/step - loss: 0.1849 - accuracy: 0.9356 - val_loss: 2.3787 - val_accuracy: 0.6917\n",
      "Epoch 6/100\n",
      "902/903 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9448\n",
      "Epoch 00006: val_accuracy did not improve from 0.72006\n",
      "903/903 [==============================] - 39s 43ms/step - loss: 0.1575 - accuracy: 0.9448 - val_loss: 2.3474 - val_accuracy: 0.7019\n",
      "Epoch 7/100\n",
      "902/903 [============================>.] - ETA: 0s - loss: 0.1354 - accuracy: 0.9530\n",
      "Epoch 00007: val_accuracy did not improve from 0.72006\n",
      "903/903 [==============================] - 40s 44ms/step - loss: 0.1354 - accuracy: 0.9530 - val_loss: 3.1088 - val_accuracy: 0.6978\n",
      "Epoch 00007: early stopping\n",
      "accuracy with val_idx=8 is 0.7794871794871795\n",
      "Starting loop with val_idx=9\n",
      "Found 90030 images belonging to 10 classes.\n",
      "Found 9612 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.6130 - accuracy: 0.7920\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.72566, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi9\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi9/assets\n",
      "901/901 [==============================] - 43s 47ms/step - loss: 0.6130 - accuracy: 0.7920 - val_loss: 0.9584 - val_accuracy: 0.7257\n",
      "Epoch 2/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.8830\n",
      "Epoch 00002: val_accuracy improved from 0.72566 to 0.74116, saving model to data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi9\n",
      "INFO:tensorflow:Assets written to: data/vgg_saved_models/model.Sr16000Cs16000Ol75Vi9/assets\n",
      "901/901 [==============================] - 41s 45ms/step - loss: 0.3405 - accuracy: 0.8830 - val_loss: 0.9050 - val_accuracy: 0.7412\n",
      "Epoch 3/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9100\n",
      "Epoch 00003: val_accuracy did not improve from 0.74116\n",
      "901/901 [==============================] - 38s 42ms/step - loss: 0.2609 - accuracy: 0.9100 - val_loss: 1.0962 - val_accuracy: 0.7017\n",
      "Epoch 4/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9252\n",
      "Epoch 00004: val_accuracy did not improve from 0.74116\n",
      "901/901 [==============================] - 39s 43ms/step - loss: 0.2158 - accuracy: 0.9252 - val_loss: 1.1109 - val_accuracy: 0.7334\n",
      "Epoch 5/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 0.9352\n",
      "Epoch 00005: val_accuracy did not improve from 0.74116\n",
      "901/901 [==============================] - 38s 42ms/step - loss: 0.1858 - accuracy: 0.9352 - val_loss: 1.2838 - val_accuracy: 0.7176\n",
      "Epoch 6/100\n",
      "901/901 [==============================] - ETA: 0s - loss: 0.1605 - accuracy: 0.9437\n",
      "Epoch 00006: val_accuracy did not improve from 0.74116\n",
      "901/901 [==============================] - 38s 42ms/step - loss: 0.1605 - accuracy: 0.9437 - val_loss: 1.3002 - val_accuracy: 0.6961\n",
      "Epoch 7/100\n",
      "900/901 [============================>.] - ETA: 0s - loss: 0.1420 - accuracy: 0.9500\n",
      "Epoch 00007: val_accuracy did not improve from 0.74116\n",
      "901/901 [==============================] - 38s 42ms/step - loss: 0.1420 - accuracy: 0.9500 - val_loss: 1.7032 - val_accuracy: 0.6863\n",
      "Epoch 00007: early stopping\n",
      "accuracy with val_idx=9 is 0.8281444582814446\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "accuracies = []\n",
    "confusion_matrices = []\n",
    "for val_idx in range(1, 10):\n",
    "    print(f\"Starting loop with val_idx={val_idx}\")\n",
    "\n",
    "    copy_to_train_test_dir(test_fold=val_idx+1)\n",
    "    model = build_vgg_model(input_shape=(72, 72, 3))\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "    val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR, target_size=(72, 72), batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        TEST_DIR,\n",
    "        #target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        #target_size=(224, 224),\n",
    "        target_size=(72, 72),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    cpcallback = ModelCheckpoint(\n",
    "        monitor=\"val_accuracy\",\n",
    "        filepath=f\"{VGG_MODEL_FILE}Vi{val_idx}\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks[0] = cpcallback\n",
    "    hist = model.fit(\n",
    "        train_generator,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # load the model with the best weights\n",
    "    model = load_model(f\"{VGG_MODEL_FILE}Vi{val_idx}\")\n",
    "\n",
    "    y_pred = model.predict(validation_generator)\n",
    "\n",
    "#     with (FOLD_DATA_DIR / f\"{val_idx+1}\").open(\"br\") as f:\n",
    "#         x, y, cl = pickle.load(f)\n",
    "    cl = fold_chunk_lens[val_idx]\n",
    "\n",
    "    y_pred_agg = sum_rule_agg(y_pred, cl)\n",
    "    y_test_agg = sum_rule_agg(to_categorical(validation_generator.classes, num_classes=NUM_CLASSES), cl)\n",
    "\n",
    "    acc = (y_pred_agg == y_test_agg).sum() / len(y_pred_agg)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"accuracy with val_idx={val_idx} is {acc}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test_agg, y_pred_agg, num_classes=NUM_CLASSES)\n",
    "    confusion_matrices.append(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8cf8709-9b4f-4a78-9a31-1464b8d9795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.748780487804878,\n",
       " 0.6429378531073446,\n",
       " 0.7114164904862579,\n",
       " 0.8020361990950227,\n",
       " 0.7312661498708011,\n",
       " 0.7231920199501247,\n",
       " 0.7433510638297872,\n",
       " 0.7794871794871795,\n",
       " 0.8281444582814446]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "879a7406-d40d-4caf-ac8f-a183f68f5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop with val_idx=1\n",
      "Found 9936 images belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 15:30:56.171291: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-22 15:31:03.150213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10380 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n",
      "2022-04-22 15:31:03.151124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1523 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\n",
      "2022-04-22 15:31:03.151691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 223 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "2022-04-22 15:31:03.152688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10399 MB memory:  -> device: 3, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\n",
      "2022-04-22 15:31:07.993809: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with val_idx=1 is 0.748780487804878\n",
      "Starting loop with val_idx=2\n",
      "Found 10809 images belonging to 10 classes.\n",
      "accuracy with val_idx=2 is 0.6429378531073446\n",
      "Starting loop with val_idx=3\n",
      "Found 11131 images belonging to 10 classes.\n",
      "accuracy with val_idx=3 is 0.7114164904862579\n",
      "Starting loop with val_idx=4\n",
      "Found 10468 images belonging to 10 classes.\n",
      "accuracy with val_idx=4 is 0.8020361990950227\n",
      "Starting loop with val_idx=5\n",
      "Found 9368 images belonging to 10 classes.\n",
      "accuracy with val_idx=5 is 0.7312661498708011\n",
      "Starting loop with val_idx=6\n",
      "Found 9738 images belonging to 10 classes.\n",
      "accuracy with val_idx=6 is 0.7231920199501247\n",
      "Starting loop with val_idx=7\n",
      "Found 9106 images belonging to 10 classes.\n",
      "accuracy with val_idx=7 is 0.7433510638297872\n",
      "Starting loop with val_idx=8\n",
      "Found 9427 images belonging to 10 classes.\n",
      "accuracy with val_idx=8 is 0.7794871794871795\n",
      "Starting loop with val_idx=9\n",
      "Found 9612 images belonging to 10 classes.\n",
      "accuracy with val_idx=9 is 0.8281444582814446\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "accuracies = []\n",
    "confusion_matrices = []\n",
    "for val_idx in range(10):\n",
    "    print(f\"Starting loop with val_idx={val_idx}\")\n",
    "\n",
    "    copy_to_train_test_dir(test_fold=val_idx+1)\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        TEST_DIR,\n",
    "        #target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        #target_size=(224, 224),\n",
    "        target_size=(72, 72),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # load the model with the best weights\n",
    "    model = load_model(f\"{VGG_MODEL_FILE}Vi{val_idx}\")\n",
    "\n",
    "    y_pred = model.predict(validation_generator)\n",
    "\n",
    "#     with (FOLD_DATA_DIR / f\"{val_idx+1}\").open(\"br\") as f:\n",
    "#         x, y, cl = pickle.load(f)\n",
    "    cl = fold_chunk_lens[val_idx]\n",
    "\n",
    "    y_pred_agg = sum_rule_agg(y_pred, cl)\n",
    "    y_test_agg = sum_rule_agg(to_categorical(validation_generator.classes, num_classes=NUM_CLASSES), cl)\n",
    "\n",
    "    acc = (y_pred_agg == y_test_agg).sum() / len(y_pred_agg)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"accuracy with val_idx={val_idx} is {acc}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test_agg, y_pred_agg, num_classes=NUM_CLASSES)\n",
    "    confusion_matrices.append(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7058c9c2-b438-429e-8a7d-d8cf6009ffdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7456235446569823, 0.05118349004535925)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies), np.std(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d42f1-384e-40db-8500-59b5c7ca40e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2d.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
